{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd88c264-4b76-4973-9426-d19b865cba6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Connecting and Mounting Blob Storage\n",
    "storageAccountName = os.getenv(\"account_name\")\n",
    "storageAccountAccessKey =  os.getenv(\"account_key\")\n",
    "# sasToken = <sas-token>\n",
    "blobContainerName = os.getenv(\"container_name\")\n",
    "mountPoint = \"/mnt/data/\"\n",
    "if not any(mount.mountPoint == mountPoint for mount in dbutils.fs.mounts()):\n",
    "  try:\n",
    "    dbutils.fs.mount(\n",
    "      source = \"wasbs://{}@{}.blob.core.windows.net\".format(blobContainerName, storageAccountName),\n",
    "      mount_point = mountPoint,\n",
    "      extra_configs = {'fs.azure.account.key.' + storageAccountName + '.blob.core.windows.net': storageAccountAccessKey}\n",
    "    #   extra_configs = {'fs.azure.sas.' + blobContainerName + '.' + storageAccountName + '.blob.core.windows.net': sasToken}\n",
    "    )\n",
    "    print(\"mount succeeded!\")\n",
    "  except Exception as e:\n",
    "    print(\"mount exception\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cd87f2e-a4ae-4c5e-b2a6-7ee3e0c7e624",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b71c3143-d2d9-4300-a0d2-d796ecfd3de9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/mnt/data/glucose_dimensional_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a32f2db9-7994-406a-b702-6131aaa81105",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "connectionString=os.getenv(\"EVENT_HUB_CONNECTION_STR\")\n",
    "\n",
    "FullConnectionString = f\"{connectionString};EntityPath=increasing_trend_alert\"\n",
    "\n",
    "encrypted_conn_string = spark._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(FullConnectionString)\n",
    "\n",
    "ehConfTrend = {\n",
    "    \"eventhubs.connectionString\": encrypted_conn_string,\n",
    "    \"eventhubs.consumerGroup\": \"sparksql\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "915a014d-99d7-4858-9544-ae238cf3b49e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+\n",
      "|user_id|         avg_week1|         avg_week2|         avg_week3|         avg_week4|         avg_week5|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+\n",
      "|    128|108.87064797537667|110.04942648751395|110.06106240408761|110.28838566371373|111.82491302490234|\n",
      "|    134|109.31297302246094|109.60140773228237|109.98912702287946|110.04472351074219| 111.5343132019043|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"GlucoseTrendAnalysis\").getOrCreate()\n",
    "\n",
    "# Loading the data from Azure Blob storage\n",
    "df_glucose = spark.read.parquet(\"dbfs:/mnt/data/glucose_dimensional_model/fact_glucose_reading.parquet\")\n",
    "df_time = spark.read.parquet(\"dbfs:/mnt/data/glucose_dimensional_model/dim_time.parquet\")\n",
    "\n",
    "# Registering the DataFrames as temporary views so they can be queried  with SQL\n",
    "df_glucose.createOrReplaceTempView(\"fact_glucose_reading\")\n",
    "df_time.createOrReplaceTempView(\"dim_time\")\n",
    "\n",
    "# As we scheduled the job on the 1st of every month, we use datetime in other to ensure that we are getting the data for the previous month\n",
    "\n",
    "current_date = datetime.now()\n",
    "first_day_of_current_month = current_date.replace(day=1)\n",
    "last_day_of_previous_month = first_day_of_current_month - timedelta(days=1)\n",
    "previous_month = last_day_of_previous_month.month\n",
    "year = last_day_of_previous_month.year\n",
    "\n",
    "# The SQL query to get the weekly average glucose levels for the previous month\n",
    "sql_query = f\"\"\"\n",
    "WITH WeeklyAverages AS (\n",
    "  SELECT \n",
    "    user_id,\n",
    "    YEAR(full_date) AS year,\n",
    "    MONTH(full_date) AS month,\n",
    "    CEIL(DAY(full_date) / 7.0) AS week_of_month,\n",
    "    AVG(avg_glucose) AS avg_weekly_glucose\n",
    "  FROM fact_glucose_reading\n",
    "  JOIN dim_time ON fact_glucose_reading.date_key = dim_time.date_key\n",
    "  WHERE YEAR(full_date) = {year} AND MONTH(full_date) = {previous_month}\n",
    "  GROUP BY user_id, YEAR(full_date), MONTH(full_date), CEIL(DAY(full_date) / 7.0)\n",
    "),\n",
    "ConsecutiveIncrease AS (\n",
    "  SELECT \n",
    "    user_id,\n",
    "    year,\n",
    "    month,\n",
    "    week_of_month,\n",
    "    avg_weekly_glucose,\n",
    "    LAG(avg_weekly_glucose, 1) OVER (PARTITION BY user_id ORDER BY year, month, week_of_month) AS prev_week_glucose,\n",
    "    CASE \n",
    "      WHEN avg_weekly_glucose > LAG(avg_weekly_glucose, 1) OVER (PARTITION BY user_id ORDER BY year, month, week_of_month) THEN 1\n",
    "      ELSE 0\n",
    "    END AS is_increasing\n",
    "  FROM WeeklyAverages\n",
    "),\n",
    "UsersWithIncrease AS (\n",
    "  SELECT \n",
    "    user_id\n",
    "  FROM ConsecutiveIncrease\n",
    "  WHERE month = {previous_month} AND year = {year}\n",
    "  GROUP BY user_id\n",
    "  HAVING SUM(is_increasing) = 4\n",
    ")\n",
    "SELECT \n",
    "  a.user_id,\n",
    "  MAX(CASE WHEN a.week_of_month = 1 THEN a.avg_weekly_glucose ELSE NULL END) AS avg_week1,\n",
    "  MAX(CASE WHEN a.week_of_month = 2 THEN a.avg_weekly_glucose ELSE NULL END) AS avg_week2,\n",
    "  MAX(CASE WHEN a.week_of_month = 3 THEN a.avg_weekly_glucose ELSE NULL END) AS avg_week3,\n",
    "  MAX(CASE WHEN a.week_of_month = 4 THEN a.avg_weekly_glucose ELSE NULL END) AS avg_week4,\n",
    "  MAX(CASE WHEN a.week_of_month = 5 THEN a.avg_weekly_glucose ELSE NULL END) AS avg_week5\n",
    "FROM WeeklyAverages a\n",
    "JOIN UsersWithIncrease b ON a.user_id = b.user_id\n",
    "GROUP BY a.user_id\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "result_df = spark.sql(sql_query)\n",
    "\n",
    "# Show the results\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aa910e6-b27f-4471-a841-5c860b968323",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checking if the DataFrame is non-empty\n",
    "if result_df.count() > 0:\n",
    "    increasing_trend_json_df = result_df.select(\n",
    "        col(\"user_id\").cast(\"string\").alias(\"partitionKey\"),\n",
    "        to_json(struct(*[col(x) for x in result_df.columns])).alias(\"body\")\n",
    "    )\n",
    "\n",
    "    # Preparing And Sending the data to Azure Event Hub\n",
    "    increasing_trend_json_df\\\n",
    "        .write \\\n",
    "        .format(\"eventhubs\") \\\n",
    "        .options(**ehConfTrend) \\\n",
    "        .save()\n",
    "else:\n",
    "    print(f\"No users with 4 consecutive weeks of increasing glucose readings for the period {year}-{previous_month}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04425fa7-2eda-4399-9385-ab6f31c71287",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.unmount('/mnt/data')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Trend Analysis",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
